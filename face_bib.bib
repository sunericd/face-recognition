
@article{yang_two-dimensional_2004,
	title = {Two-dimensional {PCA}: a new approach to appearance-based face representation and recognition},
	volume = {26},
	issn = {0162-8828},
	shorttitle = {Two-dimensional {PCA}},
	doi = {10.1109/TPAMI.2004.1261097},
	abstract = {In this paper, a new technique coined two-dimensional principal component analysis (2DPCA) is developed for image representation. As opposed to PCA, 2DPCA is based on 2D image matrices rather than 1D vectors so the image matrix does not need to be transformed into a vector prior to feature extraction. Instead, an image covariance matrix is constructed directly using the original image matrices, and its eigenvectors are derived for image feature extraction. To test 2DPCA and evaluate its performance, a series of experiments were performed on three face image databases: ORL, AR, and Yale face databases. The recognition rate across all trials was higher using 2DPCA than PCA. The experimental results also indicated that the extraction of image features is computationally more efficient using 2DPCA than PCA.},
	number = {1},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Yang, Jian and Zhang, D. and Frangi, A. F. and Yang, Jing-yu},
	month = jan,
	year = {2004},
	keywords = {visual databases, Image databases, feature extraction, Face detection, Humans, Algorithms, Computer Simulation, Female, Male, eigenfaces, eigenvectors, Face recognition, face recognition, Principal Component Analysis, Feature extraction, Principal component analysis, 2D image matrices, AR database, Artificial Intelligence, Covariance matrix, Face, face image databases, face representation, image covariance matrix, Image Enhancement, Image Interpretation, Computer-Assisted, image representation, Image representation, Independent component analysis, Information Storage and Retrieval, Kernel, Lighting, Numerical Analysis, Computer-Assisted, ORL database, Pattern Recognition, Automated, principal component analysis, Reproducibility of Results, Sensitivity and Specificity, Signal Processing, Computer-Assisted, Subtraction Technique, two-dimensional PCA, Yale face database},
	pages = {131--137},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edsun\\Zotero\\storage\\F3KB32HE\\1261097.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\edsun\\Zotero\\storage\\CRWILIJH\\Yang et al. - 2004 - Two-dimensional PCA a new approach to appearance-.pdf:application/pdf}
}

@article{zhang_face_1997,
	title = {Face recognition: eigenface, elastic matching, and neural nets},
	volume = {85},
	issn = {0018-9219},
	shorttitle = {Face recognition},
	doi = {10.1109/5.628712},
	abstract = {This paper is a comparative study of three recently proposed algorithms for face recognition: eigenface, autoassociation and classification neural nets, and elastic matching. After these algorithms were analyzed under a common statistical decision framework, they were evaluated experimentally on four individual data bases, each with a moderate subject size, and a combined data base with more than a hundred different subjects. Analysis and experimental results indicate that the eigenface algorithm, which is essentially a minimum distance classifier, works well when lighting variation is small. Its performance deteriorates significantly as lighting variation increases. The elastic matching algorithm, on the other hand, is insensitive to lighting, face position, and expression variations and therefore is more versatile. The performance of the autoassociation and classification nets is upper bounded by that of the eigenface but is more difficult to implement in practice.},
	number = {9},
	journal = {Proceedings of the IEEE},
	author = {Zhang, Jun and Yan, Yong and Lades, M.},
	month = sep,
	year = {1997},
	keywords = {neural nets, neural networks, Face recognition, face recognition, Neural networks, pattern recognition, Algorithm design and analysis, Authentication, autoassociation, Banking, classification neural nets, eigenface, elastic matching, image matching, Laboratories, Law enforcement, lighting variation, Pattern matching, performance, Signal analysis, statistical decision framework, Wavelet analysis},
	pages = {1423--1435},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edsun\\Zotero\\storage\\WEMNZVX7\\628712.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\edsun\\Zotero\\storage\\GAE9R3PE\\Zhang et al. - 1997 - Face recognition eigenface, elastic matching, and.pdf:application/pdf}
}

@article{naseem_linear_2010,
	title = {Linear {Regression} for {Face} {Recognition}},
	volume = {32},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2010.128},
	abstract = {In this paper, we present a novel approach of face identification by formulating the pattern recognition problem in terms of linear regression. Using a fundamental concept that patterns from a single-object class lie on a linear subspace, we develop a linear model representing a probe image as a linear combination of class-specific galleries. The inverse problem is solved using the least-squares method and the decision is ruled in favor of the class with the minimum reconstruction error. The proposed Linear Regression Classification (LRC) algorithm falls in the category of nearest subspace classification. The algorithm is extensively evaluated on several standard databases under a number of exemplary evaluation protocols reported in the face recognition literature. A comparative study with state-of-the-art algorithms clearly reflects the efficacy of the proposed approach. For the problem of contiguous occlusion, we propose a Modular LRC approach, introducing a novel Distance-based Evidence Fusion (DEF) algorithm. The proposed methodology achieves the best results ever reported for the challenging problem of scarf occlusion.},
	number = {11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Naseem, I. and Togneri, R. and Bennamoun, M.},
	month = nov,
	year = {2010},
	keywords = {image classification, Humans, Algorithms, Female, Male, Face recognition, face recognition, regression analysis, Inverse problems, Linear Models, Artificial Intelligence, Face, Image Enhancement, Image Interpretation, Computer-Assisted, Pattern Recognition, Automated, Biometry, Classification algorithms, Databases, distance-based evidence fusion algorithm, Image reconstruction, inverse problem, inverse problems, least squares approximations, Least squares methods, Least-Squares Analysis, least-squares method, linear regression, Linear regression, linear regression classification algorithm, minimum reconstruction error, modular LRC approach, nearest subspace classification, nearest subspace classification., Pattern recognition, pattern recognition problem, Probes, Protocols},
	pages = {2106--2112},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edsun\\Zotero\\storage\\DGTUBYHP\\5506092.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\edsun\\Zotero\\storage\\DXEV5YDJ\\Naseem et al. - 2010 - Linear Regression for Face Recognition.pdf:application/pdf}
}

@inproceedings{kazemi_one_2014,
	address = {Columbus, OH},
	title = {One millisecond face alignment with an ensemble of regression trees},
	isbn = {978-1-4799-5118-5},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909637},
	doi = {10.1109/CVPR.2014.241},
	abstract = {This paper addresses the problem of Face Alignment for a single image. We show how an ensemble of regression trees can be used to estimate the face’s landmark positions directly from a sparse subset of pixel intensities, achieving super-realtime performance with high quality predictions. We present a general framework based on gradient boosting for learning an ensemble of regression trees that optimizes the sum of square error loss and naturally handles missing or partially labelled data. We show how using appropriate priors exploiting the structure of image data helps with efﬁcient feature selection. Different regularization strategies and its importance to combat overﬁtting are also investigated. In addition, we analyse the effect of the quantity of training data on the accuracy of the predictions and explore the effect of data augmentation using synthesized data.},
	language = {en},
	urldate = {2018-12-06},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Kazemi, Vahid and Sullivan, Josephine},
	month = jun,
	year = {2014},
	pages = {1867--1874},
	file = {Kazemi and Sullivan - 2014 - One millisecond face alignment with an ensemble of.pdf:C\:\\Users\\edsun\\Zotero\\storage\\CSEEARIN\\Kazemi and Sullivan - 2014 - One millisecond face alignment with an ensemble of.pdf:application/pdf}
}

@article{cao_face_2014,
	title = {Face {Alignment} by {Explicit} {Shape} {Regression}},
	volume = {107},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-013-0667-3},
	doi = {10.1007/s11263-013-0667-3},
	abstract = {We present a very efficient, highly accurate, “Explicit Shape Regression” approach for face alignment. Unlike previous regression-based approaches, we directly learn a vectorial regression function to infer the whole facial shape (a set of facial landmarks) from the image and explicitly minimize the alignment errors over the training data. The inherent shape constraint is naturally encoded into the regressor in a cascaded learning framework and applied from coarse to fine during the test, without using a fixed parametric shape model as in most previous methods. To make the regression more effective and efficient, we design a two-level boosted regression, shape indexed features and a correlation-based feature selection method. This combination enables us to learn accurate models from large training data in a short time (20 min for 2,000 training images), and run regression extremely fast in test (15 ms for a 87 landmarks shape). Experiments on challenging data show that our approach significantly outperforms the state-of-the-art in terms of both accuracy and efficiency.},
	language = {en},
	number = {2},
	urldate = {2018-12-06},
	journal = {International Journal of Computer Vision},
	author = {Cao, Xudong and Wei, Yichen and Wen, Fang and Sun, Jian},
	month = apr,
	year = {2014},
	keywords = {Correlation based feature selection, Face alignment, Non-parametric shape constraint, Shape indexed feature, Tow-level boosted regression},
	pages = {177--190},
	file = {Springer Full Text PDF:C\:\\Users\\edsun\\Zotero\\storage\\D7CYMVLE\\Cao et al. - 2014 - Face Alignment by Explicit Shape Regression.pdf:application/pdf}
}

@inproceedings{hinterstoisser_simultaneous_2008,
	address = {Leeds},
	title = {Simultaneous {Recognition} and {Homography} {Extraction} of {Local} {Patches} with a {Simple} {Linear} {Classifier}},
	isbn = {978-1-901725-36-0},
	url = {http://www.bmva.org/bmvc/2008/papers/16.html},
	doi = {10.5244/C.22.10},
	abstract = {We show that the simultaneous estimation of keypoint identities and poses is more reliable than the two separate steps undertaken by previous approaches. A simple linear classiﬁer coupled with linear predictors trained during a learning phase appears to be sufﬁcient for this task. The retrieved poses are subpixel accurate due to the linear predictors. We demonstrate the advantages of our approach on real-time 3D object detection and tracking applications. Thanks to the high accuracy, one single keypoint is often enough to precisely estimate the object pose. As a result, we can deal in real-time with objects that are signiﬁcantly less textured than the ones required by state-of-the-art methods.},
	language = {en},
	urldate = {2018-12-07},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2008},
	publisher = {British Machine Vision Association},
	author = {Hinterstoisser, S. and Benhimane, S. and Lepetit, V. and Fua, P. and Navab, N.},
	year = {2008},
	pages = {10.1--10.10},
	file = {Hinterstoisser et al. - 2008 - Simultaneous Recognition and Homography Extraction.pdf:C\:\\Users\\edsun\\Zotero\\storage\\IPIY7L8E\\Hinterstoisser et al. - 2008 - Simultaneous Recognition and Homography Extraction.pdf:application/pdf}
}

@book{hartley_multiple_2003,
	title = {Multiple {View} {Geometry} in {Computer} {Vision}},
	isbn = {978-0-521-54051-3},
	abstract = {A basic problem in computer vision is to understand the structure of a real world scene given several images of it. Techniques for solving this problem are taken from projective geometry and photogrammetry. Here, the authors cover the geometric principles and their algebraic representation in terms of camera projection matrices, the fundamental matrix and the trifocal tensor. The theory and methods of computation of these entities are discussed with real examples, as is their use in the reconstruction of scenes from multiple images. The new edition features an extended introduction covering the key ideas in the book (which itself has been updated with additional examples and appendices) and significant new results which have appeared since the first edition. Comprehensive background material is provided, so readers familiar with linear algebra and basic numerical methods can understand the projective geometry and estimation algorithms presented, and implement the algorithms directly from the book.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Hartley, Richard and Zisserman, Andrew},
	year = {2003},
	note = {Google-Books-ID: si3R3Pfa98QC},
	keywords = {Mathematics / Applied, Computers / Computer Graphics, Computers / Computer Science, Computers / Computer Vision \& Pattern Recognition, Mathematics / Geometry / General, Technology \& Engineering / Robotics}
}

@article{pal_review_1993,
	title = {A review on image segmentation techniques},
	volume = {26},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/003132039390135J},
	doi = {10.1016/0031-3203(93)90135-J},
	abstract = {Many image segmentation techniques are available in the literature. Some of these techniques use only the gray level histogram, some use spatial details while others use fuzzy set theoretic approaches. Most of these techniques are not suitable for noisy environments. Some works have been done using the Markov Random Field (MRF) model which is robust to noise, but is computationally involved. Neural network architectures which help to get the output in real time because of their parallel processing ability, have also been used for segmentation and they work fine even when the noise level is very high. The literature on color image segmentation is not that rich as it is for gray tone images. This paper critically reviews and summarizes some of these techniques. Attempts have been made to cover both fuzzy and non-fuzzy techniques including color image segmentation and neural network based approaches. Adequate attention is paid to segmentation of range images and magnetic resonance images. It also addresses the issue of quantitative evaluation of segmentation results.},
	number = {9},
	urldate = {2018-12-08},
	journal = {Pattern Recognition},
	author = {Pal, Nikhil R and Pal, Sankar K},
	month = sep,
	year = {1993},
	keywords = {Clustering, Edge detection, Fuzzy sets, Image segmentation, Markov Random Field, Relaxation, Thresholding},
	pages = {1277--1294},
	file = {ScienceDirect Snapshot:C\:\\Users\\edsun\\Zotero\\storage\\465YBPTC\\003132039390135J.html:text/html}
}

@book{szeliski_computer_2010,
	title = {Computer {Vision}: {Algorithms} and {Applications}},
	isbn = {978-1-84882-935-0},
	shorttitle = {Computer {Vision}},
	abstract = {Humans perceive the three-dimensional structure of the world with apparent ease. However, despite all of the recent advances in computer vision research, the dream of having a computer interpret an image at the same level as a two-year old remains elusive. Why is computer vision such a challenging problem and what is the current state of the art? Computer Vision: Algorithms and Applications explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both for specialized applications such as medical imaging, and for fun, consumer-level tasks such as image editing and stitching, which students can apply to their own personal photos and videos. More than just a source of “recipes,” this exceptionally authoritative and comprehensive textbook/reference also takes a scientific approach to basic vision problems, formulating physical models of the imaging process before inverting them to produce descriptions of a scene. These problems are also analyzed using statistical models and solved using rigorous engineering techniques Topics and features:  Structured to support active curricula and project-oriented courses, with tips in the Introduction for using the book in a variety of customized courses Presents exercises at the end of each chapter with a heavy emphasis on testing algorithms and containing numerous suggestions for small mid-term projects Provides additional material and more detailed mathematical topics in the Appendices, which cover linear algebra, numerical techniques, and Bayesian estimation theory Suggests additional reading at the end of each chapter, including the latest research in each sub-field, in addition to a full Bibliography at the end of the book Supplies supplementary course material for students at the associated website, http://szeliski.org/Book/  Suitable for an upper-level undergraduate or graduate-level course in computer science or engineering, this textbook focuses on basic techniques that work under real-world conditions and encourages students to push their creative boundaries. Its design and exposition also make it eminently suitable as a unique reference to the fundamental techniques and current research literature in computer vision.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Szeliski, Richard},
	month = sep,
	year = {2010},
	note = {Google-Books-ID: bXzAlkODwa8C},
	keywords = {Computers / Computer Graphics, Computers / Optical Data Processing, Computers / Software Development \& Engineering / General}
}

@article{lecun_handwritten_nodate,
	title = {Handwritten {Digit} {Recognition} with a {Back}-{Propagation} {Network}},
	abstract = {We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1\% error rate and about a 9\% reject rate on zipcode digits provided by the U.S. Postal Service.},
	language = {en},
	author = {LeCun, Yann and Boser, Bernhard E and Denker, John S and Henderson, Donnie and Howard, R E and Hubbard, Wayne E and Jackel, Lawrence D},
	pages = {9},
	file = {LeCun et al. - Handwritten Digit Recognition with a Back-Propagat.pdf:C\:\\Users\\edsun\\Zotero\\storage\\5W8YHUGA\\LeCun et al. - Handwritten Digit Recognition with a Back-Propagat.pdf:application/pdf}
}

@article{gerbrands_relationships_1981,
	series = {1980 {Conference} on {Pattern} {Recognition}},
	title = {On the relationships between {SVD}, {KLT} and {PCA}},
	volume = {14},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/0031320381900820},
	doi = {10.1016/0031-3203(81)90082-0},
	abstract = {In recent literature on digital image processing much attention is devoted to the singular value decomposition (SVD) of a matrix. Many authors refer to the Karhunen-Loeve transform (KLT) and principal components analysis (PCA) while treating the SVD. In this paper we give definitions of the three transforms and investigate their relationships. It is shown that in the context of multivariate statistical analysis and statistical pattern recognition the three transforms are very similar if a specific estimate of the column covariance matrix is used. In the context of two-dimensional image processing this similarity still holds if one single matrix is considered. In that approach the use of the names KLT and PCA is rather inappropriate and confusing. If the matrix is considered to be a realization of a two-dimensional random process, the SVD and the two statistically defined transforms differ substantially.},
	number = {1},
	urldate = {2018-12-08},
	journal = {Pattern Recognition},
	author = {Gerbrands, Jan J.},
	month = jan,
	year = {1981},
	keywords = {Image processing, Karhunen-Loeve transform, Orthogonal image transforms, Principal components, Singular value decomposition, Statistical analysis, Statistical pattern recognition},
	pages = {375--381},
	file = {ScienceDirect Snapshot:C\:\\Users\\edsun\\Zotero\\storage\\K2ESC8D5\\0031320381900820.html:text/html}
}

@article{moore_principal_1981,
	title = {Principal component analysis in linear systems: {Controllability}, observability, and model reduction},
	volume = {26},
	issn = {0018-9286},
	shorttitle = {Principal component analysis in linear systems},
	doi = {10.1109/TAC.1981.1102568},
	abstract = {Kalman's minimal realization theory involves geometric objects (controllable, unobservable subspaces) which are subject to structural instability. Specifically, arbitrarily small perturbations in a model may cause a change in the dimensions of the associated subspaces. This situation is manifested in computational difficulties which arise in attempts to apply textbook algorithms for computing a minimal realization. Structural instability associated with geometric theories is not unique to control; it arises in the theory of linear equations as well. In this setting, the computational problems have been studied for decades and excellent tools have been developed for coping with the situation. One of the main goals of this paper is to call attention to principal component analysis (Hotelling, 1933), and an algorithm (Golub and Reinsch, 1970) for computing the singular value decompositon of a matrix. Together they form a powerful tool for coping with structural instability in dynamic systems. As developed in this paper, principal component analysis is a technique for analyzing signals. (Singular value decomposition provides the computational machinery.) For this reason, Kalman's minimal realization theory is recast in terms of responses to injected signals. Application of the signal analysis to controllability and observability leads to a coordinate system in which the "internally balanced" model has special properties. For asymptotically stable systems, this yields working approximations of{\textless}tex{\textgreater}X\_c, X\_{\textbackslash}baro{\textless}/tex{\textgreater}, the controllable and unobservable subspaces. It is proposed that a natural first step in model reduction is to apply the mechanics of minimal realization using these working subspaces.},
	number = {1},
	journal = {IEEE Transactions on Automatic Control},
	author = {Moore, B.},
	month = feb,
	year = {1981},
	keywords = {Principal component analysis, Signal analysis, Singular value decomposition, Controllability, Controllability, linear systems, Equations, Kalman filters, Linear systems, Matrix decomposition, Multivariable systems, Observability, Observability, linear systems, Reduced order systems, Reduced-order systems, linear},
	pages = {17--32},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edsun\\Zotero\\storage\\7LE7TL33\\1102568.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\edsun\\Zotero\\storage\\JZUC5MYJ\\Moore - 1981 - Principal component analysis in linear systems Co.pdf:application/pdf}
}

@incollection{kaya_basic_1972,
	title = {A {Basic} {Study} on {Human} {Face} {Recognition}},
	isbn = {978-0-12-737140-5},
	url = {http://www.sciencedirect.com/science/article/pii/B9780127371405500178},
	abstract = {This chapter discusses a basic study on human face recognition. The ultimate objective of the study is to recognize human faces, three dimensional ever-changing patterns. Difficulties of human face recognition lie in the following two properties of human face as a pattern: (1) number of patterns, that is, faces to be classified is tremendous, maybe infinite; and (2) almost all patterns are very similar. Any normal face has two eyes, one nose, one mouth and two ears which are similarly located in a face. It goes without saying that the ability of identifying a person by his face is indispensable to any person in his daily life. A machine possessing this ability, if realizable, may be used as a guardman of private facilities, such as a factory, a parking place, or a nightclub. It will help police activities by finding the photograph of the corresponding suspect in a pile of photographs.},
	urldate = {2018-12-08},
	booktitle = {Frontiers of {Pattern} {Recognition}},
	publisher = {Academic Press},
	author = {Kaya, Y. and Kobayashi, K.},
	editor = {Watanabe, Satosi},
	month = jan,
	year = {1972},
	doi = {10.1016/B978-0-12-737140-5.50017-8},
	pages = {265--289},
	file = {ScienceDirect Snapshot:C\:\\Users\\edsun\\Zotero\\storage\\2FC2B5PE\\B9780127371405500178.html:text/html}
}

@article{golub_singular_nodate,
	title = {Singular value decomposition and least squares solutions},
	language = {en},
	author = {Golub, G H and Reinsch, C},
	pages = {18},
	file = {Golub and Reinsch - Singular value decomposition and least squares sol.pdf:C\:\\Users\\edsun\\Zotero\\storage\\747WBJB6\\Golub and Reinsch - Singular value decomposition and least squares sol.pdf:application/pdf}
}

@article{tolba_face_2006,
	title = {Face {Recognition}: {A} {Literature} {Review}},
	volume = {2},
	shorttitle = {{FACE} {RECOGNITION}},
	url = {https://www.sid.ir/En/Journal/ViewPaper.aspx?ID=382432},
	abstract = {Download Free Full-Text of an article FACE RECOGNITION: A LITERATURE REVIEW},
	language = {En},
	number = {2},
	urldate = {2018-12-09},
	journal = {International Journal of Signal Processing},
	author = {Tolba, A.H. and Baz, A.L. and Harby, El},
	month = jan,
	year = {2006},
	pages = {88--103},
	file = {Snapshot:C\:\\Users\\edsun\\Zotero\\storage\\MFA8TT29\\ViewPaper.html:text/html}
}

@article{galton_personal_1888,
	title = {Personal {Identification} and {Description}},
	url = {http://ci.nii.ac.jp/naid/10022086353/},
	urldate = {2018-12-09},
	journal = {Nature},
	author = {Galton, F.},
	year = {1888},
	file = {Personal Identification and Description Snapshot:C\:\\Users\\edsun\\Zotero\\storage\\GUE46CIJ\\10022086353.html:text/html}
}

@inproceedings{turk_face_1991,
	title = {Face recognition using eigenfaces},
	doi = {10.1109/CVPR.1991.139758},
	abstract = {An approach to the detection and identification of human faces is presented, and a working, near-real-time face recognition system which tracks a subject's head and then recognizes the person by comparing characteristics of the face to those of known individuals is described. This approach treats face recognition as a two-dimensional recognition problem, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. Face images are projected onto a feature space ('face space') that best encodes the variation among known face images. The face space is defined by the 'eigenfaces', which are the eigenvectors of the set of faces; they do not necessarily correspond to isolated features such as eyes, ears, and noses. The framework provides the ability to learn to recognize new faces in an unsupervised manner.{\textless}{\textless}ETX{\textgreater}{\textgreater}},
	booktitle = {1991 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Proceedings}},
	author = {Turk, M. A. and Pentland, A. P.},
	month = jun,
	year = {1991},
	keywords = {Face detection, Humans, Character recognition, Computational modeling, Computer vision, computerised pattern recognition, eigenfaces, eigenvalues and eigenfunctions, eigenvectors, Eyes, face images, Face recognition, face recognition system, face space, feature space, Head, human faces, Image recognition, Nose, two-dimensional recognition, unsupervised learning},
	pages = {586--591},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edsun\\Zotero\\storage\\8SQ9NGMS\\139758.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\edsun\\Zotero\\storage\\UYACGR55\\Turk and Pentland - 1991 - Face recognition using eigenfaces.pdf:application/pdf}
}

@article{daspremont_direct_nodate,
	title = {A {Direct} {Formulation} for {Sparse} {PCA} {Using} {Semidefinite} {Programming}},
	abstract = {We examine the problem of approximating, in the Frobenius-norm sense, a positive, semideﬁnite symmetric matrix by a rank-one matrix, with an upper bound on the cardinality of its eigenvector. The problem arises in the decomposition of a covariance matrix into sparse factors, and has wide applications ranging from biology to ﬁnance. We use a modiﬁcation of the classical variational representation of the largest eigenvalue of a symmetric matrix, where cardinality is constrained, and derive a semideﬁnite programming based relaxation for our problem.},
	language = {en},
	author = {D'aspremont, Alexandre and Ghaoui, Laurent E and Jordan, Michael I and Lanckriet, Gert R},
	pages = {8},
	file = {D'aspremont et al. - A Direct Formulation for Sparse PCA Using Semidefi.pdf:C\:\\Users\\edsun\\Zotero\\storage\\H4J3JIEH\\D'aspremont et al. - A Direct Formulation for Sparse PCA Using Semidefi.pdf:application/pdf}
}

@article{zou_sparse_2006,
	title = {Sparse {Principal} {Component} {Analysis}},
	volume = {15},
	issn = {1061-8600},
	url = {http://amstat.tandfonline.com/doi/abs/10.1198/106186006X113430},
	doi = {10.1198/106186006X113430},
	abstract = {Principal component analysis (PCA) is widely used in data processing and dimensionality reduction. However, PCA suffers from the fact that each principal component is a linear combination of all the original variables, thus it is often difficult to interpret the results. We introduce a new method called sparse principal component analysis (SPCA) using the lasso (elastic net) to produce modified principal components with sparse loadings. We first show that PCA can be formulated as a regression-type optimization problem; sparse loadings are then obtained by imposing the lasso (elastic net) constraint on the regression coefficients. Efficient algorithms are proposed to fit our SPCA models for both regular multivariate data and gene expression arrays. We also give a new formula to compute the total variance of modified principal components. As illustrations, SPCA is applied to real and simulated data with encouraging results.},
	number = {2},
	urldate = {2018-12-10},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
	month = jun,
	year = {2006},
	pages = {265--286},
	file = {Snapshot:C\:\\Users\\edsun\\Zotero\\storage\\URNZSEWP\\106186006X113430.html:text/html;Submitted Version:C\:\\Users\\edsun\\Zotero\\storage\\NZ4N6N9T\\Zou et al. - 2006 - Sparse Principal Component Analysis.pdf:application/pdf}
}

@article{wang_2dpca_2013,
	title = {2DPCA with {L}1-norm for simultaneously robust and sparse modelling},
	volume = {46},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608013001640},
	doi = {10.1016/j.neunet.2013.06.002},
	abstract = {Robust dimensionality reduction is an important issue in processing multivariate data. Two-dimensional principal component analysis based on L1-norm (2DPCA-L1) is a recently developed technique for robust dimensionality reduction in the image domain. The basis vectors of 2DPCA-L1, however, are still dense. It is beneficial to perform a sparse modelling for the image analysis. In this paper, we propose a new dimensionality reduction method, referred to as 2DPCA-L1 with sparsity (2DPCAL1-S), which effectively combines the robustness of 2DPCA-L1 and the sparsity-inducing lasso regularization. It is a sparse variant of 2DPCA-L1 for unsupervised learning. We elaborately design an iterative algorithm to compute the basis vectors of 2DPCAL1-S. The experiments on image data sets confirm the effectiveness of the proposed approach.},
	urldate = {2018-12-11},
	journal = {Neural Networks},
	author = {Wang, Haixian and Wang, Jing},
	month = oct,
	year = {2013},
	keywords = {2DPCA-L1, Dimensionality reduction, Lasso regularization, Robust modelling},
	pages = {190--198},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\edsun\\Zotero\\storage\\F8R2FSVJ\\Wang and Wang - 2013 - 2DPCA with L1-norm for simultaneously robust and s.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\edsun\\Zotero\\storage\\5G72T2ME\\S0893608013001640.html:text/html}
}

@article{vines_simple_2000,
	title = {Simple principal components},
	volume = {49},
	copyright = {2000 Royal Statistical Society},
	issn = {1467-9876},
	url = {http://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9876.00204},
	doi = {10.1111/1467-9876.00204},
	abstract = {We introduce an algorithm for producing simple approximate principal components directly from a variance–covariance matrix. At the heart of the algorithm is a series of ‘simplicity preserving’ linear transformations. Each transformation seeks a direction within a two-dimensional subspace that has maximum variance. However, the choice of directions is limited so that the direction can be represented by a vector of integers whenever the subspace can also be represented by vector if integers. The resulting approximate components can therefore always be represented by integers. Furthermore the elements of these integer vectors are often small, particularly for the first few components. We demonstrate the performance of this algorithm on two data sets and show that good approximations to the principal components that are also clearly simple and interpretable can result.},
	language = {en},
	number = {4},
	urldate = {2018-12-11},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	author = {Vines, S. K.},
	month = jan,
	year = {2000},
	keywords = {Interpretation, Pairwise linear transformation, Principal components analysis, Simplification},
	pages = {441--451},
	file = {Full Text PDF:C\:\\Users\\edsun\\Zotero\\storage\\YDCSGNMM\\Vines - 2000 - Simple principal components.pdf:application/pdf;Snapshot:C\:\\Users\\edsun\\Zotero\\storage\\4DYUA88W\\1467-9876.html:text/html}
}

@inproceedings{mairal_online_2009,
	address = {New York, NY, USA},
	series = {{ICML} '09},
	title = {Online {Dictionary} {Learning} for {Sparse} {Coding}},
	isbn = {978-1-60558-516-1},
	url = {http://doi.acm.org/10.1145/1553374.1553463},
	doi = {10.1145/1553374.1553463},
	abstract = {Sparse coding---that is, modelling data vectors as sparse linear combinations of basis elements---is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on learning the basis set, also called dictionary, to adapt it to specific data, an approach that has recently proven to be very effective for signal reconstruction and classification in the audio and image processing domains. This paper proposes a new online optimization algorithm for dictionary learning, based on stochastic approximations, which scales up gracefully to large datasets with millions of training samples. A proof of convergence is presented, along with experiments with natural images demonstrating that it leads to faster performance and better dictionaries than classical batch algorithms for both small and large datasets.},
	urldate = {2018-12-12},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
	year = {2009},
	pages = {689--696},
	file = {ACM Full Text PDF:C\:\\Users\\edsun\\Zotero\\storage\\TH3W5YJF\\Mairal et al. - 2009 - Online Dictionary Learning for Sparse Coding.pdf:application/pdf}
}

@article{lee_learning_1999,
	title = {Learning the parts of objects by non-negative matrix factorization},
	volume = {401},
	issn = {1476-4687},
	url = {http://www.nature.com/articles/44565},
	doi = {10.1038/44565},
	abstract = {Is perception of the whole based on perception of its parts? There is psychological1 and physiological2,3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4,5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
	language = {en},
	number = {6755},
	urldate = {2018-12-12},
	journal = {Nature},
	author = {Lee, Daniel D. and Seung, H. Sebastian},
	month = oct,
	year = {1999},
	pages = {788--791},
	file = {Full Text PDF:C\:\\Users\\edsun\\Zotero\\storage\\5DV4TBFY\\Lee and Seung - 1999 - Learning the parts of objects by non-negative matr.pdf:application/pdf;Snapshot:C\:\\Users\\edsun\\Zotero\\storage\\4B3RX7AW\\44565.html:text/html}
}

@article{lee_algorithms_nodate,
	title = {Algorithms for {Non}-negative {Matrix} {Factorization}},
	abstract = {Non-negative matrix factorization (NMF) has previously been shown to be a useful decomposition for multivariate data. Two different multiplicative algorithms for NMF are analyzed. They differ only slightly in the multiplicative factor used in the update rules. One algorithm can be shown to minimize the conventional least squares error while the other minimizes the generalized Kullback-Leibler divergence. The monotonic convergence of both algorithms can be proven using an auxiliary function analogous to that used for proving convergence of the ExpectationMaximization algorithm. The algorithms can also be interpreted as diagonally rescaled gradient descent, where the rescaling factor is optimally chosen to ensure convergence.},
	language = {en},
	author = {Lee, Daniel D and Seung, H Sebastian},
	pages = {7},
	file = {Lee and Seung - Algorithms for Non-negative Matrix Factorization.pdf:C\:\\Users\\edsun\\Zotero\\storage\\ICZ5ZFHM\\Lee and Seung - Algorithms for Non-negative Matrix Factorization.pdf:application/pdf}
}

@article{devarajan_nonnegative_2008,
	title = {Nonnegative {Matrix} {Factorization}: {An} {Analytical} and {Interpretive} {Tool} in {Computational} {Biology}},
	volume = {4},
	issn = {1553-7358},
	shorttitle = {Nonnegative {Matrix} {Factorization}},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000029},
	doi = {10.1371/journal.pcbi.1000029},
	abstract = {In the last decade, advances in high-throughput technologies such as DNA microarrays have made it possible to simultaneously measure the expression levels of tens of thousands of genes and proteins. This has resulted in large amounts of biological data requiring analysis and interpretation. Nonnegative matrix factorization (NMF) was introduced as an unsupervised, parts-based learning paradigm involving the decomposition of a nonnegative matrix V into two nonnegative matrices, W and H, via a multiplicative updates algorithm. In the context of a p×n gene expression matrix V consisting of observations on p genes from n samples, each column of W defines a metagene, and each column of H represents the metagene expression pattern of the corresponding sample. NMF has been primarily applied in an unsupervised setting in image and natural language processing. More recently, it has been successfully utilized in a variety of applications in computational biology. Examples include molecular pattern discovery, class comparison and prediction, cross-platform and cross-species analysis, functional characterization of genes and biomedical informatics. In this paper, we review this method as a data analytical and interpretive tool in computational biology with an emphasis on these applications.},
	language = {en},
	number = {7},
	urldate = {2018-12-12},
	journal = {PLOS Computational Biology},
	author = {Devarajan, Karthik},
	month = jul,
	year = {2008},
	keywords = {Gene expression, Algorithms, Face recognition, Microarrays, Acute myeloid leukemia, Computational biology, Leukemias, Pattern recognition receptors},
	pages = {e1000029},
	file = {Full Text PDF:C\:\\Users\\edsun\\Zotero\\storage\\4PDQI2GW\\Devarajan - 2008 - Nonnegative Matrix Factorization An Analytical an.pdf:application/pdf;Snapshot:C\:\\Users\\edsun\\Zotero\\storage\\7FHHPGXI\\article.html:text/html}
}

@inproceedings{szegedy_going_2015,
	title = {Going {Deeper} {With} {Convolutions}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html},
	urldate = {2018-12-13},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	year = {2015},
	pages = {1--9},
	file = {Full Text PDF:C\:\\Users\\edsun\\Zotero\\storage\\VTXMEYNU\\Szegedy et al. - 2015 - Going Deeper With Convolutions.pdf:application/pdf;Snapshot:C\:\\Users\\edsun\\Zotero\\storage\\6HFSXZUW\\Szegedy_Going_Deeper_With_2015_CVPR_paper.html:text/html}
}